# Aulas de Código

09. Desenvolvendo um gerador de Hashes
    * *Conferir Código:* **09_gerador_hashes.py**

10. Desenvolvendo um gerador de Wordlists
    * *Conferir Código:* **10_gerador_wordists.py**

11. Desenvolvendo um Web Scraping
    * *Conferir Código:* **11_scrapper.py**

12. Desenvolvendo um Web Crawler
    * *Conferir Código:* **12_web_crawler.py**

---

## Complemento:

* Aula 10:

### Wordlists

Wordlists em Python são listas de palavras ou sequências de caracteres usadas para processamento de texto, testes de segurança ou geração de dados fictícios. Elas podem ser armazenadas em arquivos ou em listas em memória, e são facilmente manipuladas e acessadas em Python.

### Biblioteca **itertools**
A biblioteca itertools em Python permite criar e manipular iteradores de forma eficiente. Ela oferece funções para combinações, permutações, agrupamento de elementos e geração de sequências infinitas. É útil para tarefas que envolvem iteração avançada sobre elementos.

---

* Aula 11:

### Web Scraping

Web scraping é o processo de extrair dados de sites da web de maneira automatizada. Ele envolve o uso de software para percorrer e analisar o código-fonte de uma página da web, coletando informações específicas, como texto, imagens, links e muito mais. Esses dados podem ser posteriormente utilizados para diversas finalidades, como análise de mercado, pesquisa, monitoramento de preços, entre outros. O web scraping é uma técnica poderosa para acessar e obter informações disponíveis na web de forma eficiente e automatizada.

Bibliotecas Requests e BeautifulSoup

A biblioteca **requests** é usada para enviar solicitações HTTP em Python. Ela permite que você envie solicitações para URLs e receba as respostas, seja para obter o conteúdo de uma página da web, enviar dados de formulário ou realizar outras interações com servidores da web.

Por outro lado, a biblioteca **BeautifulSoup** é usada para analisar e extrair dados de documentos HTML e XML. Ela fornece métodos e estruturas de dados convenientes para navegar e pesquisar o código-fonte de uma página da web, facilitando a extração de informações específicas, como texto, links, imagens, entre outros.

---

* Aula 12:

### Web Crawler

Web crawler, também conhecido como spider, é um programa automatizado que percorre a web de forma sistemática, visitando e analisando páginas da web de maneira recursiva. O objetivo principal de um web crawler é coletar informações de várias páginas da web, indexá-las e permitir a recuperação eficiente dessas informações posteriormente.

Bibliotecas Operator e Collections

A biblioteca operator fornece funções eficientes para executar operações comuns em objetos, como adição, subtração, comparação, acesso a atributos, entre outros. Ela é útil para simplificar e otimizar a manipulação de dados em Python, principalmente quando trabalhamos com estruturas de dados complexas.

Por outro lado, a biblioteca collections oferece implementações adicionais de estruturas de dados especializadas que vão além das oferecidas pelas estruturas de dados nativas do Python. Ela inclui estruturas de dados como defaultdict, Counter, deque, OrderedDict e outras, que são projetadas para atender a necessidades específicas, como contar elementos, armazenar itens em uma ordem específica, criar dicionários com valores padrão e muito mais.

---